# GroundingGPT:Language-Enhanced Multi-modal Grounding Model

<a href='https://lzw-lzw.github.io/GroundingGPT.github.io/'><img src='https://img.shields.io/badge/Project-Page-Green'></a>  <a href='https://arxiv.org/abs/2401.06071'><img src='https://img.shields.io/badge/Paper-Arxiv-red'></a> 

<p align="center">
    <img src="images/logo.png" width="20%"> <br>
</p>

## Introduction
GroundingGPT is an end-to-end multimodal grounding model that accurately comprehends inputs and possesses robust grounding capabilities across multi modalities,including images, audios, and videos. To address the issue of limited data, we construct a diverse and high-quality multimodal training dataset. This dataset encompasses a rich collection of multimodal data enriched with spatial and temporal information, thereby serving as a valuable resource to foster further advancements in this field. Extensive experimental evaluations validate the effectiveness of the GroundingGPT model in understanding and grounding tasks across various modalities. 

More details are available in our [project page](https://lzw-lzw.github.io/GroundingGPT.github.io/). 

<p align="center">
    <img src="images/architecture.png" width="80%"> <br>
    The overall structure of GroundingGPT. Blue boxes represent video as input, while yellow boxes represent image as input.
</p>

## Release
We will soon open-source our datasets, codes and models, stay tuned!


## Content

## Demo


## Acknowledgement
- [LLaVA](https://github.com/haotian-liu/LLaVA)
- [Video-LLaMA](https://github.com/DAMO-NLP-SG/Video-LLaMA)
- [Shikra](https://github.com/shikras/shikra)
